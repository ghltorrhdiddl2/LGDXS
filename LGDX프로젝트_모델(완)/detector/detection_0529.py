# detector/detection.py
# ê°œì„ ëœ ì½”ë“œ êµ¬ì¡°: ì§§ì€ ì˜ìƒì—ì„œë„ ì •í™•í•˜ê²Œ ë¶„ì„ë˜ë„ë¡ ìŠ¬ë¼ì´ë”© ë²„í¼ ë°©ì‹ ì ìš© + Gemini ë¶„ì„ ìµœì í™”

import os
import time
import cv2
import numpy as np
from datetime import datetime
from pathlib import Path
from ultralytics import YOLO
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
import google.generativeai as genai
import re
import warnings
from PIL import Image
from io import BytesIO

warnings.filterwarnings('ignore', category=UserWarning)

GOOGLE_API_KEY = "AIzaSyAodNAwhpYmQkLWPA3dv-giw0WppjLhjMY"
genai.configure(api_key=GOOGLE_API_KEY)
LLM = genai.GenerativeModel(model_name="models/gemini-2.0-flash")

PROJECT_ROOT = Path(__file__).resolve().parent
USE_CAMERA = False
CAMERA_ID = 0
VIDEO_PATH = "./videos/aggressive_dog.mp4"

FRAME_SKIP = 3
WINDOW_SIZE = 5
DETECTION_WINDOWS = 3
ALLOWED_NORMAL = 1
FPS = 30.0
GEMINI_BUFFER_SECONDS = 5
DB_BUFFER_SECONDS = 5
GEMINI_MAX_FRAMES = int(FPS * GEMINI_BUFFER_SECONDS)
DB_MAX_FRAMES = int(FPS * DB_BUFFER_SECONDS)
STD_MULTIPLIER = 0.3
IF_CONTAM = 0.2
LOF_CONTAM = 0.2
LOF_NEIGHBORS = 4
dog_name = "ë³„ì´"

PROMPT = """ë‹¹ì‹ ì€ ê°•ì•„ì§€ì˜ ì´ìƒí–‰ë™ì„ ê°ë…í•˜ëŠ” ìˆ˜ì˜ì‚¬ì…ë‹ˆë‹¤.
ê°•ì•„ì§€ì˜ í˜„ì¬ í–‰ë™ì„ ë¶„ì„í•˜ê³  ë³´í˜¸ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”. 

3ì¤„ ì´ë‚´ë¡œ ë‹¤ìŒì„ ì„¤ëª…í•´ì£¼ì„¸ìš”:
1. ì§€ê¸ˆ ë¬´ì—‡ì„ í•˜ê³  ìˆëŠ”ì§€
2. ë³´í˜¸ìê°€ ì–´ë–»ê²Œ ëŒ€ì²˜í•´ì•¼ í•˜ëŠ”ì§€
3. ì‹¬ê°ë„: [0/1/2/3]ë‹¨ê³„ ì¤‘ ì„ íƒ

* ìœ„ê¸‰ ìƒí™©ì¼ ë•Œë§Œ **êµµê²Œ í‘œì‹œ**í•´
* ì˜ˆì˜ì ì¸ í‘œí˜„ì€ ìƒëµ"""

MODEL_PATH = PROJECT_ROOT / "dog_pose_model.pt"
yolo = YOLO(str(MODEL_PATH))
if_model  = IsolationForest(contamination=IF_CONTAM, random_state=42)
lof_model = LocalOutlierFactor(n_neighbors=LOF_NEIGHBORS, contamination=LOF_CONTAM, novelty=True)

# ì¤‘ìš” í‚¤ì›Œë“œ ì •ì˜
CRITICAL_KEYWORDS = [
    "ìœ„í—˜", "ì‘ê¸‰", "ê²½ë ¨", "ë°œì‘", "ì¹˜ë£Œ", "ì¦‰ì‹œ", "ë³‘ì›", "ê³ í†µ", "ì‹¬ê°", "ìœ„ê¸‰",
    "ë¶ˆì•ˆ", "ê³µê²©", "ë¬´ê¸°ë ¥", "íƒˆì§„", "íƒˆìˆ˜", "êµ¬í† ", "ì„¤ì‚¬", "í˜¸í¡", "ë§ˆë¹„", "ê²½ì§"
]

def z_norm(a):
    return (a - a.mean()) / (a.std() or 1.0)

def summarize_action(max_movement, avg_movement, movement_description):
    prompt = f"""
ë‹¹ì‹ ì€ ê°•ì•„ì§€ì˜ ì´ìƒí–‰ë™ì„ ê°ë…í•˜ëŠ” ìˆ˜ì˜ì‚¬ì…ë‹ˆë‹¤.
3ì¤„ ì´ë‚´ë¡œ ë‹¤ìŒì„ ì„¤ëª…í•´ì£¼ì„¸ìš”:
1. ì§€ê¸ˆ ë¬´ì—‡ì„ í•˜ê³  ìˆëŠ”ì§€ (ì˜ˆ: ë‹¤ë¦¬ë¥¼ ì‹¬í•˜ê²Œ í”ë“¤ê³  ìˆìŒ)
2. ë³´í˜¸ìê°€ ì–´ë–»ê²Œ ëŒ€ì²˜í•´ì•¼ í•˜ëŠ”ì§€
3. ì‹¬ê°ë„: [0/1/2/3]ë‹¨ê³„ ì¤‘ ì„ íƒ

* ìœ„ê¸‰ ìƒí™©ì¼ ë•Œë§Œ **êµµê²Œ í‘œì‹œ**í•´
* ì˜ˆì˜ì ì¸ í‘œí˜„ì€ ìƒëµ

[ê´€ì°° ì •ë³´]
- ìµœëŒ€ ì›€ì§ì„: {max_movement:.2f}
- í‰ê·  ì›€ì§ì„: {avg_movement:.2f}
- í–‰ë™ ìš”ì•½: {movement_description}
"""
    return prompt

def encode_frames_to_image(frames):
    try:
        if not frames:
            return None
        # ëŒ€í‘œ í”„ë ˆì„ ì„ íƒ (ë§ˆì§€ë§‰ í”„ë ˆì„)
        frame = frames[-1]
        _, img_encoded = cv2.imencode(".jpg", frame)
        
        # ì´ë¯¸ì§€ íŒŒì¼ë¡œ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        os.makedirs("gemini_input", exist_ok=True)
        image_path = f"gemini_input/analysis_frame_{timestamp}.jpg"
        cv2.imwrite(image_path, frame)
        print(f"ğŸ–¼ï¸ Gemini ì…ë ¥ ì´ë¯¸ì§€ ì €ì¥ë¨: {image_path}")
        
        return img_encoded.tobytes()
    except Exception as e:
        print("âŒ ì´ë¯¸ì§€ ì¸ì½”ë”© ì˜¤ë¥˜:", str(e))
        return None

def encode_frames_to_video(frames):
    try:
        if not frames:
            return None
            
        height, width, _ = frames[0].shape
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        os.makedirs("analyzed_clips", exist_ok=True)
        video_path = f"analyzed_clips/analysis_{timestamp}.mp4"
        
        # VideoWriter ì´ˆê¸°í™”
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(video_path, fourcc, 10.0, (width, height))
        
        # í”„ë ˆì„ë“¤ì„ ë¹„ë””ì˜¤ë¡œ ì‘ì„±
        for frame in frames:
            out.write(frame)
        out.release()
        
        return video_path
    except Exception as e:
        print("âŒ ë¹„ë””ì˜¤ ì¸ì½”ë”© ì˜¤ë¥˜:", str(e))
        return None

def analyze_with_gemini(frames_count, features_data, frames_vis):
    try:
        print("\nğŸ¤– ì œë¯¸ë‹ˆ ë¶„ì„ ì‹œì‘...")
        print(f"ğŸ“Š ë¶„ì„ í”„ë ˆì„ ìˆ˜: {len(frames_vis)} (ì•½ {len(frames_vis)/FPS:.1f}ì´ˆ)")
        max_movement = 0
        avg_movement = 0

        if len(features_data) >= WINDOW_SIZE:
            recent_features = features_data[-WINDOW_SIZE:]
            changes = []
            for i in range(1, len(recent_features)):
                prev = np.array(recent_features[i-1]).reshape(-1, 2)
                curr = np.array(recent_features[i]).reshape(-1, 2)
                distances = np.linalg.norm(curr - prev, axis=1)
                head = np.mean(distances[0:3])
                body = np.mean(distances[4:8])
                legs = np.mean(distances[8:16])
                tail = np.mean(distances[16:])
                changes.append({ 'max': np.max(distances), 'avg': np.mean(distances), 'head': head, 'body': body, 'legs': legs, 'tail': tail })

            max_movement = np.max([c['max'] for c in changes])
            avg_movement = np.mean([c['avg'] for c in changes])
            head_avg = np.mean([c['head'] for c in changes])
            body_avg = np.mean([c['body'] for c in changes])
            legs_avg = np.mean([c['legs'] for c in changes])
            tail_avg = np.mean([c['tail'] for c in changes])

        movement_description = f"ë¨¸ë¦¬: {head_avg:.2f}, ëª¸í†µ: {body_avg:.2f}, ë‹¤ë¦¬: {legs_avg:.2f}, ê¼¬ë¦¬: {tail_avg:.2f}"
        analysis_prompt = summarize_action(max_movement, avg_movement, movement_description)

        # ë¹„ë””ì˜¤ ìƒì„± ë° Geminië¡œ ì „ì†¡
        video_path = encode_frames_to_video(frames_vis)
        if not video_path:
            print("âš ï¸ ë¹„ë””ì˜¤ ìƒì„± ì‹¤íŒ¨")
            return 1, "ë¹„ë””ì˜¤ ì¸ì½”ë”© ì‹¤íŒ¨"

        print(f"ğŸ“¤ ë¹„ë””ì˜¤ íŒŒì¼ì„ Geminië¡œ ì „ì†¡ ì¤‘...")
        
        try:
            # ë¹„ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ
            video_file = genai.upload_file(path=video_path)
            
            # ë¹„ë””ì˜¤ ì²˜ë¦¬ ìƒíƒœ í™•ì¸
            while video_file.state.name == "PROCESSING":
                print("â³ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì¤‘...")
                time.sleep(2)
                video_file = genai.get_file(video_file.name)

            if video_file.state.name == "FAILED":
                raise ValueError(f"ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹¤íŒ¨: {video_file.state.name}")

            print("âœ… ë¹„ë””ì˜¤ ì²˜ë¦¬ ì™„ë£Œ")

            # Gemini ë¶„ì„ ìš”ì²­
            prompt_with_time = analysis_prompt + f" (ë¶„ì„ì‹œê°„:{datetime.now()})"
            
            # í”„ë¡¬í”„íŠ¸ ì €ì¥
            os.makedirs("gemini_input", exist_ok=True)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            with open(f"gemini_input/prompt_{timestamp}.txt", "w", encoding="utf-8") as f:
                f.write(prompt_with_time)
            
            # ë¹„ë””ì˜¤ì™€ í”„ë¡¬í”„íŠ¸ë¡œ ë¶„ì„ ìš”ì²­
            resp = LLM.generate_content([video_file, prompt_with_time])
            
            if not resp:
                print("âš ï¸ ì‘ë‹µ ì—†ìŒ")
                return 1, "ë¶„ì„ ì‹¤íŒ¨"
                
            text = resp.text.strip()
            print("\nâœ… Gemini ì‘ë‹µ:", text)
            
            # Gemini ì‘ë‹µ ì €ì¥
            with open(f"gemini_input/response_{timestamp}.txt", "w", encoding="utf-8") as f:
                f.write(text)

            stage_match = re.search(r'(\d+)ë‹¨ê³„', text)
            stage = int(stage_match.group(1)) if stage_match else 1

            return stage, text

        finally:
            # ì„ì‹œ ë¹„ë””ì˜¤ íŒŒì¼ ì‚­ì œ
            try:
                if os.path.exists(video_path):
                    os.remove(video_path)
            except Exception as e:
                print(f"âš ï¸ ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")

    except Exception as e:
        print("âŒ Gemini ì˜¤ë¥˜:", str(e))
        return 1, f"ì˜¤ë¥˜: {str(e)}"

def process_video():
    print("\nğŸ¥ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹œì‘...")
    
    video_path = str(PROJECT_ROOT / VIDEO_PATH)
    if not os.path.exists(video_path):
        print(f"âŒ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        return
        
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print("âŒ ë¹„ë””ì˜¤ ì—´ê¸° ì‹¤íŒ¨")
        return

    # ë¹„ë””ì˜¤ ì •ë³´ í™•ì¸
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    video_fps = cap.get(cv2.CAP_PROP_FPS)
    video_duration = total_frames / video_fps
    
    print(f"\nğŸ“Š ë¹„ë””ì˜¤ ì •ë³´:")
    print(f"- ì´ í”„ë ˆì„: {total_frames}")
    print(f"- FPS: {video_fps:.1f}")
    print(f"- ê¸¸ì´: {video_duration:.1f}ì´ˆ")
    
    # ì§§ì€ ì˜ìƒ ê°ì§€ ë° íŒŒë¼ë¯¸í„° ì¡°ì •
    if video_duration < 10:  # 10ì´ˆ ë¯¸ë§Œ ì˜ìƒ
        print("\nâš ï¸ ì§§ì€ ì˜ìƒ ê°ì§€ - íŒŒë¼ë¯¸í„° ì¡°ì •")
        global FRAME_SKIP, WINDOW_SIZE, DETECTION_WINDOWS, GEMINI_BUFFER_SECONDS
        FRAME_SKIP = 2
        WINDOW_SIZE = 3
        DETECTION_WINDOWS = 2
        GEMINI_BUFFER_SECONDS = min(3, video_duration * 0.5)  # ì˜ìƒ ê¸¸ì´ì˜ ì ˆë°˜ ë˜ëŠ” 3ì´ˆ
        print(f"- í”„ë ˆì„ ìŠ¤í‚µ: {FRAME_SKIP}")
        print(f"- ìœˆë„ìš° í¬ê¸°: {WINDOW_SIZE}")
        print(f"- ê°ì§€ ìœˆë„ìš°: {DETECTION_WINDOWS}")
        print(f"- ë¶„ì„ ì‹œê°„: {GEMINI_BUFFER_SECONDS:.1f}ì´ˆ")

    # ë²„í¼ ì´ˆê¸°í™”
    features = []          # íŠ¹ì§•ì  ë°ì´í„°
    frames_vis = []        # ì œë¯¸ë‹ˆ ë¶„ì„ìš© ì›ë³¸ í”„ë ˆì„
    frames_display = []    # í™”ë©´ í‘œì‹œìš© í”„ë ˆì„
    abnormal_windows = []
    continuous_detection = False
    last_analysis_time = time.time()
    frame_count = 0
    
    # ì¤‘ìš” ë¶„ì„ ê²°ê³¼ ì €ì¥
    important_analyses = []  # (ì‹¬ê°ë„, ì‹œê°„, ë‚´ìš©) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
    max_severity = 0
    max_severity_text = ""

    cv2.namedWindow("Dog Pose Detection", cv2.WINDOW_NORMAL)

    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                # ë§ˆì§€ë§‰ í”„ë ˆì„ì—ì„œ ë‚¨ì€ ë°ì´í„° ë¶„ì„
                if len(frames_vis) > WINDOW_SIZE:
                    print("\nğŸ” ì˜ìƒ ì¢…ë£Œ - ìµœì¢… ë¶„ì„ ì‹¤í–‰")
                    stage, summary = analyze_with_gemini(len(frames_vis), features, frames_vis)
                    
                    # ë¶„ì„ ê²°ê³¼ ì €ì¥
                    timestamp = datetime.now().strftime("%H:%M:%S")
                    important_analyses.append((stage, timestamp, summary))
                    if stage > max_severity:
                        max_severity = stage
                        max_severity_text = summary
                print("\nğŸ¬ ë¹„ë””ì˜¤ ì¢…ë£Œ")
                break

            if frame is None:
                continue

            frame_count += 1
            if frame_count % FRAME_SKIP != 0:
                continue

            # YOLO ë¶„ì„
            result = yolo.predict(frame, conf=0.7, verbose=False)[0]
            vis = result.plot()

            if result.keypoints is not None and len(result.keypoints.data) > 0:
                kpts = result.keypoints.data[0]
                if kpts.shape[0] == 24:
                    # í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ ë° íŠ¹ì§•ì  ê³„ì‚°
                    pelvis = kpts[14][:2]
                    l_sh, r_sh = kpts[11][:2], kpts[12][:2]
                    scale = np.linalg.norm(l_sh - r_sh) or 1.0
                    
                    feat = []
                    for x, y, v in kpts:
                        feat += [(x - pelvis[0]) / scale, (y - pelvis[1]) / scale] if v >= 0.5 else [0.0, 0.0]
                    
                    # ë²„í¼ ê´€ë¦¬
                    features.append(feat)
                    frames_vis.append(frame.copy())
                    frames_display.append(vis.copy())
                    
                    # ë²„í¼ í¬ê¸° ê´€ë¦¬ - ìµœì†Œ í•„ìš” í”„ë ˆì„ ìˆ˜ í™•ë³´
                    min_required_frames = int(FPS * GEMINI_BUFFER_SECONDS / FRAME_SKIP)
                    max_frames = min_required_frames + 10  # ì—¬ìœ ë¶„

                    # ì´ìƒí–‰ë™ ê°ì§€
                    if len(features) >= WINDOW_SIZE:
                        X = np.array(features[-WINDOW_SIZE:])
                        if_model.fit(X)
                        lof_model.fit(X)
                        combined = (z_norm(if_model.decision_function(X)) + z_norm(lof_model.decision_function(X))) / 2.0
                        threshold = combined.mean() - STD_MULTIPLIER * combined.std()
                        is_abnormal = combined[-1] < threshold

                        abnormal_windows.append(is_abnormal)
                        if len(abnormal_windows) > DETECTION_WINDOWS:
                            abnormal_windows.pop(0)

                        # ì´ìƒí–‰ë™ ì²´í¬ ë° ë¶„ì„
                        current_time = time.time()
                        enough_frames = len(frames_vis) >= min_required_frames
                        if (sum(abnormal_windows) >= (DETECTION_WINDOWS - ALLOWED_NORMAL) and 
                            not continuous_detection and 
                            enough_frames and
                            current_time - last_analysis_time >= GEMINI_BUFFER_SECONDS):
                            
                            continuous_detection = True
                            print(f"\nğŸ” í–‰ë™ ë¶„ì„ ì‹œì‘ (í”„ë ˆì„ ìˆ˜: {len(frames_vis)})")
                            
                            stage, summary = analyze_with_gemini(len(frames_vis), features, frames_vis)
                            
                            # ë¶„ì„ ê²°ê³¼ ì €ì¥
                            timestamp = datetime.now().strftime("%H:%M:%S")
                            
                            # ì¤‘ìš” í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆê±°ë‚˜ ì‹¬ê°ë„ê°€ ë†’ì€ ê²½ìš° ì €ì¥
                            has_critical_keyword = any(keyword in summary for keyword in CRITICAL_KEYWORDS)
                            if has_critical_keyword or stage >= 2:
                                important_analyses.append((stage, timestamp, summary))
                            
                            # ìµœëŒ€ ì‹¬ê°ë„ ì—…ë°ì´íŠ¸
                            if stage > max_severity:
                                max_severity = stage
                                max_severity_text = summary
                            
                            features.clear()
                            frames_vis.clear()
                            frames_display.clear()
                            abnormal_windows.clear()
                            continuous_detection = False
                            last_analysis_time = current_time

                    # ë²„í¼ê°€ ë„ˆë¬´ ì»¤ì§€ë©´ ì˜¤ë˜ëœ í”„ë ˆì„ ì œê±°
                    if len(features) > max_frames:
                        features.pop(0)
                        frames_vis.pop(0)
                        frames_display.pop(0)

            # í™”ë©´ í‘œì‹œ
            if len(frames_display) > 0:
                cv2.imshow("Dog Pose Detection", frames_display[-1])
            else:
                cv2.imshow("Dog Pose Detection", vis)

            if cv2.waitKey(1) & 0xFF == ord('q'):
                print("\nğŸ‘‹ ë¶„ì„ ì¢…ë£Œ")
                if important_analyses:
                    print_final_analysis(important_analyses, max_severity, max_severity_text)
                break

    finally:
        if important_analyses:
            print_final_analysis(important_analyses, max_severity, max_severity_text)
        cap.release()
        cv2.destroyAllWindows()

def print_final_analysis(important_analyses, max_severity, max_severity_text):
    severity_emoji = {0: "âœ…", 1: "âš ï¸", 2: "ğŸš¨", 3: "â›”"}
    severity_text = {0: "ì •ìƒ", 1: "ê²½ë¯¸", 2: "ì£¼ì˜", 3: "ìœ„í—˜"}
    
    print(f"\nğŸ“Š ìµœì¢… ë¶„ì„ ìš”ì•½")
    print("=" * 40)
    
    # 1. ìµœê³  ì‹¬ê°ë„ ìƒí™©
    print(f"\n{severity_emoji[max_severity]} ê°€ì¥ ìœ„í—˜í•œ ìƒí™©: {severity_text[max_severity]}ë‹¨ê³„")
    print(f"ğŸ’¬ {max_severity_text}")
    
    # 2. ì£¼ìš” ìœ„í—˜ ì¦ìƒ (ìƒìœ„ 3ê°œë§Œ)
    keyword_counts = {}
    for _, _, content in important_analyses:
        for keyword in CRITICAL_KEYWORDS:
            if keyword in content:
                keyword_counts[keyword] = keyword_counts.get(keyword, 0) + 1
    
    if keyword_counts:
        print("\nğŸš¨ ì£¼ìš” ë°œê²¬ ì¦ìƒ:")
        # ë¹ˆë„ìˆ˜ ê¸°ì¤€ ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
        sorted_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:3]
        for keyword, count in sorted_keywords:
            print(f"- {keyword}: {count}íšŒ")
    
    # 3. í–‰ë™ ë¶„ì„ ë° ì†Œê²¬
    print("\nğŸ“‹ í–‰ë™ ë¶„ì„ ë° ì†Œê²¬:")
    
    # ê°€ì¥ ìµœê·¼ì˜ ì‹¬ê°í•œ í–‰ë™ë“¤ ìˆ˜ì§‘ (ì‹¬ê°ë„ 2 ì´ìƒ)
    critical_behaviors = []
    for stage, _, content in sorted(important_analyses, key=lambda x: x[1], reverse=True):
        if stage >= 2:
            # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì—¬ ì²« ë¬¸ì¥ë§Œ ì‚¬ìš©
            first_sentence = content.split('.')[0].strip()
            if first_sentence and len(first_sentence) > 5:  # ì˜ë¯¸ ìˆëŠ” ë¬¸ì¥ì¸ì§€ í™•ì¸
                critical_behaviors.append(first_sentence)
                if len(critical_behaviors) >= 2:  # ìµœëŒ€ 2ê°œì˜ ì£¼ìš” í–‰ë™ë§Œ í‘œì‹œ
                    break
    
    # í–‰ë™ ë¶„ì„ í‘œì‹œ
    if critical_behaviors:
        print("ë°œê²¬ëœ ì£¼ìš” ì´ìƒí–‰ë™:")
        for behavior in critical_behaviors:
            print(f"- {behavior}")
    
    # ê¶Œì¥ ì‚¬í•­
    print("\nê¶Œì¥ ì‚¬í•­:")
    if max_severity >= 3:
        print("â›” ì‘ê¸‰ ì§„ë£Œê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    elif max_severity >= 2:
        print("â— ìˆ˜ì˜ì‚¬ ìƒë‹´ì´ ê¶Œì¥ë©ë‹ˆë‹¤.")
    elif max_severity >= 1:
        print("âš ï¸ ì§€ì†ì ì¸ ê´€ì°°ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    else:
        print("âœ… íŠ¹ë³„í•œ ì´ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
    
    print("=" * 40)

if __name__ == "__main__":
    process_video()
